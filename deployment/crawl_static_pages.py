#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é™æ€ç½‘é¡µçˆ¬å–å·¥å…·

åŠŸèƒ½ï¼š
1. å¯åŠ¨ Flask å¼€å‘æœåŠ¡å™¨
2. çˆ¬å–æ‰€æœ‰é¡µé¢å¹¶ä¿å­˜ä¸ºé™æ€ HTML
3. ä¸‹è½½æ‰€æœ‰é™æ€èµ„æºï¼ˆCSS, JS, å›¾ç‰‡ç­‰ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 crawl_static_pages.py
"""

import os
import sys
import time
import subprocess
import requests
import shutil
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import re

# é…ç½®
SERVER_HOST = "127.0.0.1"
SERVER_PORT = 5050
BASE_URL = f"http://{SERVER_HOST}:{SERVER_PORT}"
SOURCE_DIR = Path("/Users/weilai/Documents/devs/IFC4-3-x-development/code_zh")
OUTPUT_DIR = Path("/Users/weilai/Documents/devs/IFC-4-3-Chinese")
MAX_RETRIES = 3
TIMEOUT = 30

# éœ€è¦çˆ¬å–çš„é¡µé¢åˆ—è¡¨ï¼ˆæ ¹æ®å®é™…è·¯ç”±é…ç½®ï¼‰
PAGES_TO_CRAWL = [
    "/",
    "/content/foreword.htm",
    "/content/introduction.htm",
    "/content/scope.htm",
    "/content/normative_references.htm",
    "/content/terms_and_definitions.htm",
    "/content/abbreviated_terms.htm",
    "/content/schema_definition.htm",
]

class StaticSiteCrawler:
    """é™æ€ç½‘ç«™çˆ¬å–å™¨"""

    def __init__(self):
        self.session = requests.Session()
        self.crawled_urls = set()
        self.failed_urls = []

    def fetch_page(self, url: str, retry: int = 0) -> tuple[str, int]:
        """
        è·å–é¡µé¢å†…å®¹

        Args:
            url: é¡µé¢ URL
            retry: é‡è¯•æ¬¡æ•°

        Returns:
            (å†…å®¹, çŠ¶æ€ç ) å…ƒç»„
        """
        try:
            response = self.session.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response.text, response.status_code
        except requests.RequestException as e:
            if retry < MAX_RETRIES:
                print(f"  âš ï¸  é‡è¯• {retry + 1}/{MAX_RETRIES}: {url}")
                time.sleep(2 ** retry)  # æŒ‡æ•°é€€é¿
                return self.fetch_page(url, retry + 1)
            else:
                print(f"  âŒ å¤±è´¥: {url} - {e}")
                self.failed_urls.append((url, str(e)))
                return None, 0

    def save_page(self, url_path: str, content: str):
        """
        ä¿å­˜é¡µé¢åˆ°æœ¬åœ°æ–‡ä»¶

        Args:
            url_path: URL è·¯å¾„
            content: é¡µé¢å†…å®¹
        """
        # ç¡®å®šæ–‡ä»¶è·¯å¾„
        if url_path == "/" or url_path == "":
            file_path = OUTPUT_DIR / "index.html"
        else:
            # ç§»é™¤å‰å¯¼æ–œæ 
            url_path = url_path.lstrip("/")

            # å¦‚æœè·¯å¾„ä»¥ .htm æˆ– .html ç»“å°¾ï¼Œç›´æ¥ä½¿ç”¨
            if url_path.endswith((".htm", ".html")):
                file_path = OUTPUT_DIR / url_path
            else:
                # å¦åˆ™åˆ›å»ºç›®å½•å¹¶ä¿å­˜ä¸º index.html
                file_path = OUTPUT_DIR / url_path / "index.html"

        # åˆ›å»ºç›®å½•
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)

        print(f"  âœ… å·²ä¿å­˜: {file_path.relative_to(OUTPUT_DIR)}")

    def extract_links(self, html: str, base_url: str) -> list[str]:
        """
        ä» HTML ä¸­æå–é“¾æ¥

        Args:
            html: HTML å†…å®¹
            base_url: åŸºç¡€ URL

        Returns:
            é“¾æ¥åˆ—è¡¨
        """
        soup = BeautifulSoup(html, "html.parser")
        links = []

        # æå–æ‰€æœ‰ <a> æ ‡ç­¾çš„ href
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            # è¿‡æ»¤æ‰é”šç‚¹å’Œå¤–éƒ¨é“¾æ¥
            if not href.startswith("#") and not href.startswith("http"):
                full_url = urljoin(base_url, href)
                # åªä¿ç•™åŒåŸŸåçš„é“¾æ¥
                if urlparse(full_url).netloc == urlparse(base_url).netloc:
                    links.append(full_url)

        return links

    def crawl_recursive(self, start_url: str, max_depth: int = 5):
        """
        é€’å½’çˆ¬å–é¡µé¢

        Args:
            start_url: èµ·å§‹ URL
            max_depth: æœ€å¤§æ·±åº¦
        """
        queue = [(start_url, 0)]  # (URL, æ·±åº¦)

        while queue:
            url, depth = queue.pop(0)

            # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–æˆ–è¶…è¿‡æœ€å¤§æ·±åº¦
            if url in self.crawled_urls or depth > max_depth:
                continue

            print(f"\nğŸ“„ çˆ¬å– (æ·±åº¦ {depth}): {url}")

            # è·å–é¡µé¢
            content, status = self.fetch_page(url)
            if content is None:
                continue

            # æ ‡è®°ä¸ºå·²çˆ¬å–
            self.crawled_urls.add(url)

            # ä¿å­˜é¡µé¢
            url_path = urlparse(url).path
            self.save_page(url_path, content)

            # æå–é“¾æ¥å¹¶åŠ å…¥é˜Ÿåˆ—
            links = self.extract_links(content, url)
            for link in links:
                if link not in self.crawled_urls:
                    queue.append((link, depth + 1))

    def generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report_path = OUTPUT_DIR / "deployment" / "crawl_report.txt"
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write("é™æ€ç½‘é¡µçˆ¬å–æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æˆåŠŸçˆ¬å–: {len(self.crawled_urls)} ä¸ªé¡µé¢\n")
            f.write(f"å¤±è´¥: {len(self.failed_urls)} ä¸ªé¡µé¢\n\n")

            if self.failed_urls:
                f.write("å¤±è´¥çš„ URL:\n")
                for url, error in self.failed_urls:
                    f.write(f"  - {url}\n    é”™è¯¯: {error}\n")

        print(f"\nğŸ“Š æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def check_server_running() -> bool:
    """æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get(BASE_URL, timeout=2)
        return response.status_code < 500
    except:
        return False


def start_server() -> subprocess.Popen:
    """å¯åŠ¨ Flask æœåŠ¡å™¨"""
    print("ğŸš€ å¯åŠ¨ Flask æœåŠ¡å™¨...")

    env = os.environ.copy()
    env["IFC_LANG"] = "zh"
    env["FLASK_APP"] = "server.py"
    env["FLASK_ENV"] = "development"

    process = subprocess.Popen(
        ["python3", "-c", "from server import app; app.run(host='127.0.0.1', port=5050, debug=False)"],
        cwd=SOURCE_DIR,
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )

    # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    print("â³ ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨...")
    for i in range(30):  # æœ€å¤šç­‰å¾… 30 ç§’
        time.sleep(1)
        if check_server_running():
            print(f"âœ… æœåŠ¡å™¨å·²å¯åŠ¨: {BASE_URL}")
            return process

    # è¶…æ—¶
    process.terminate()
    raise RuntimeError("æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("IFC 4.3 ä¸­æ–‡ç‰ˆ - é™æ€ç½‘é¡µçˆ¬å–å·¥å…·")
    print("=" * 60)

    # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²è¿è¡Œ
    server_process = None
    if check_server_running():
        print(f"âœ… æœåŠ¡å™¨å·²åœ¨è¿è¡Œ: {BASE_URL}")
    else:
        try:
            server_process = start_server()
        except Exception as e:
            print(f"âŒ æ— æ³•å¯åŠ¨æœåŠ¡å™¨: {e}")
            sys.exit(1)

    try:
        # åˆ›å»ºçˆ¬è™«
        crawler = StaticSiteCrawler()

        # å¼€å§‹çˆ¬å–
        print("\n" + "=" * 60)
        print("å¼€å§‹çˆ¬å–é¡µé¢...")
        print("=" * 60)

        crawler.crawl_recursive(BASE_URL, max_depth=10)

        # ç”ŸæˆæŠ¥å‘Š
        crawler.generate_report()

        # æ€»ç»“
        print("\n" + "=" * 60)
        print("çˆ¬å–å®Œæˆï¼")
        print("=" * 60)
        print(f"âœ… æˆåŠŸ: {len(crawler.crawled_urls)} ä¸ªé¡µé¢")
        print(f"âŒ å¤±è´¥: {len(crawler.failed_urls)} ä¸ªé¡µé¢")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    finally:
        # åœæ­¢æœåŠ¡å™¨
        if server_process:
            print("\nğŸ›‘ åœæ­¢æœåŠ¡å™¨...")
            server_process.terminate()
            server_process.wait(timeout=5)
            print("âœ… æœåŠ¡å™¨å·²åœæ­¢")


if __name__ == "__main__":
    main()
