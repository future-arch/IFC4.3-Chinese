#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´é™æ€ç½‘é¡µçˆ¬å–å·¥å…· - å¢å¼ºç‰ˆ

åŠŸèƒ½ï¼š
1. å¯åŠ¨ Flask å¼€å‘æœåŠ¡å™¨
2. æ·±åº¦é€’å½’çˆ¬å–æ‰€æœ‰é¡µé¢
3. ä¸‹è½½æ‰€æœ‰é™æ€èµ„æº
4. ç”Ÿæˆè¯¦ç»†çˆ¬å–æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 crawl_complete.py
"""

import os
import sys
import time
import subprocess
import requests
import shutil
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote
from bs4 import BeautifulSoup
import re
from collections import deque

# é…ç½®
SERVER_HOST = "127.0.0.1"
SERVER_PORT = 5050
BASE_URL = f"http://{SERVER_HOST}:{SERVER_PORT}"
SOURCE_DIR = Path("/Users/weilai/Documents/devs/IFC4-3-x-development/code_zh")
OUTPUT_DIR = Path("/Users/weilai/Documents/devs/IFC-4-3-Chinese")
MAX_RETRIES = 3
TIMEOUT = 30
MAX_PAGES = 10000  # æœ€å¤§é¡µé¢æ•°é™åˆ¶

class CompleteSiteCrawler:
    """å®Œæ•´ç½‘ç«™çˆ¬å–å™¨ - å¢å¼ºç‰ˆ"""

    def __init__(self):
        self.session = requests.Session()
        self.crawled_urls = set()
        self.failed_urls = []
        self.url_queue = deque()
        self.pages_crawled = 0

    def normalize_url(self, url: str) -> str:
        """
        è§„èŒƒåŒ– URL

        Args:
            url: åŸå§‹ URL

        Returns:
            è§„èŒƒåŒ–åçš„ URL
        """
        parsed = urlparse(url)
        # ç§»é™¤ fragment (#éƒ¨åˆ†)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        return normalized

    def fetch_page(self, url: str, retry: int = 0) -> tuple:
        """
        è·å–é¡µé¢å†…å®¹

        Args:
            url: é¡µé¢ URL
            retry: é‡è¯•æ¬¡æ•°

        Returns:
            (å†…å®¹, çŠ¶æ€ç ) å…ƒç»„
        """
        try:
            response = self.session.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response.text, response.status_code
        except requests.RequestException as e:
            if retry < MAX_RETRIES:
                time.sleep(2 ** retry)  # æŒ‡æ•°é€€é¿
                return self.fetch_page(url, retry + 1)
            else:
                print(f"  âŒ å¤±è´¥: {url} - {e}")
                self.failed_urls.append((url, str(e)))
                return None, 0

    def save_page(self, url_path: str, content: str):
        """
        ä¿å­˜é¡µé¢åˆ°æœ¬åœ°æ–‡ä»¶

        Args:
            url_path: URL è·¯å¾„
            content: é¡µé¢å†…å®¹
        """
        # ç¡®å®šæ–‡ä»¶è·¯å¾„
        if url_path == "/" or url_path == "":
            file_path = OUTPUT_DIR / "index.html"
        else:
            # ç§»é™¤å‰å¯¼æ–œæ 
            url_path = url_path.lstrip("/")
            # URL è§£ç 
            url_path = unquote(url_path)

            # å¦‚æœè·¯å¾„ä»¥ .htm æˆ– .html ç»“å°¾ï¼Œç›´æ¥ä½¿ç”¨
            if url_path.endswith((".htm", ".html")):
                file_path = OUTPUT_DIR / url_path
            else:
                # å¦åˆ™åˆ›å»ºç›®å½•å¹¶ä¿å­˜ä¸º index.html
                file_path = OUTPUT_DIR / url_path / "index.html"

        # åˆ›å»ºç›®å½•
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ–‡ä»¶
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)
            print(f"  âœ… [{self.pages_crawled}] {file_path.relative_to(OUTPUT_DIR)}")
        except Exception as e:
            print(f"  âš ï¸  ä¿å­˜å¤±è´¥: {file_path} - {e}")

    def extract_links(self, html: str, base_url: str) -> list:
        """
        ä» HTML ä¸­æå–æ‰€æœ‰é“¾æ¥

        Args:
            html: HTML å†…å®¹
            base_url: åŸºç¡€ URL

        Returns:
            é“¾æ¥åˆ—è¡¨
        """
        soup = BeautifulSoup(html, "html.parser")
        links = []

        # æå–æ‰€æœ‰ <a> æ ‡ç­¾çš„ href
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]

            # è·³è¿‡é”šç‚¹ã€JavaScriptã€mailtoç­‰
            if href.startswith(("#", "javascript:", "mailto:", "tel:")):
                continue

            # è·³è¿‡å¤–éƒ¨é“¾æ¥
            if href.startswith("http") and not href.startswith(BASE_URL):
                continue

            # æ„å»ºå®Œæ•´ URL
            full_url = urljoin(base_url, href)

            # åªä¿ç•™åŒåŸŸåçš„é“¾æ¥
            if urlparse(full_url).netloc == urlparse(base_url).netloc:
                normalized = self.normalize_url(full_url)
                links.append(normalized)

        return list(set(links))  # å»é‡

    def crawl_bfs(self, start_url: str):
        """
        ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢çˆ¬å–æ‰€æœ‰é¡µé¢

        Args:
            start_url: èµ·å§‹ URL
        """
        self.url_queue.append(start_url)

        while self.url_queue and self.pages_crawled < MAX_PAGES:
            url = self.url_queue.popleft()

            # è§„èŒƒåŒ– URL
            url = self.normalize_url(url)

            # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
            if url in self.crawled_urls:
                continue

            # æ ‡è®°ä¸ºå·²çˆ¬å–
            self.crawled_urls.add(url)
            self.pages_crawled += 1

            # è·å–é¡µé¢
            print(f"\nğŸ“„ çˆ¬å– [{self.pages_crawled}/{MAX_PAGES}]: {url}")
            content, status = self.fetch_page(url)

            if content is None:
                continue

            # ä¿å­˜é¡µé¢
            url_path = urlparse(url).path
            self.save_page(url_path, content)

            # æå–é“¾æ¥å¹¶åŠ å…¥é˜Ÿåˆ—
            links = self.extract_links(content, url)
            for link in links:
                if link not in self.crawled_urls:
                    self.url_queue.append(link)

            # æ¯ 50 é¡µè¾“å‡ºè¿›åº¦
            if self.pages_crawled % 50 == 0:
                print(f"\nğŸ“Š è¿›åº¦: å·²çˆ¬å– {self.pages_crawled} é¡µï¼Œé˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(self.url_queue)} é¡µå¾…å¤„ç†")
                print(f"   å¤±è´¥: {len(self.failed_urls)} é¡µ")

    def generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report_path = OUTPUT_DIR / "deployment" / "crawl_report_complete.txt"
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("é™æ€ç½‘é¡µå®Œæ•´çˆ¬å–æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æˆåŠŸçˆ¬å–: {len(self.crawled_urls)} ä¸ªé¡µé¢\n")
            f.write(f"å¤±è´¥: {len(self.failed_urls)} ä¸ªé¡µé¢\n")
            f.write(f"æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’\n\n")

            if self.failed_urls:
                f.write("å¤±è´¥çš„ URL:\n")
                f.write("-" * 80 + "\n")
                for url, error in self.failed_urls:
                    f.write(f"  {url}\n")
                    f.write(f"    é”™è¯¯: {error}\n\n")

        print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def check_server_running() -> bool:
    """æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get(BASE_URL, timeout=2)
        return response.status_code < 500
    except:
        return False


def start_server() -> subprocess.Popen:
    """å¯åŠ¨ Flask æœåŠ¡å™¨"""
    print("ğŸš€ å¯åŠ¨ Flask æœåŠ¡å™¨...")

    env = os.environ.copy()
    env["IFC_LANG"] = "zh"
    env["FLASK_APP"] = "server.py"
    env["FLASK_ENV"] = "development"

    process = subprocess.Popen(
        ["python3", "-c", "from server import app; app.run(host='127.0.0.1', port=5050, debug=False, use_reloader=False)"],
        cwd=SOURCE_DIR,
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )

    # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    print("â³ ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨...")
    for i in range(30):  # æœ€å¤šç­‰å¾… 30 ç§’
        time.sleep(1)
        if check_server_running():
            print(f"âœ… æœåŠ¡å™¨å·²å¯åŠ¨: {BASE_URL}")
            return process

    # è¶…æ—¶
    process.terminate()
    raise RuntimeError("æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶")


def main():
    """ä¸»å‡½æ•°"""
    global start_time
    start_time = time.time()

    print("=" * 80)
    print("IFC 4.3 ä¸­æ–‡ç‰ˆ - å®Œæ•´é™æ€ç½‘é¡µçˆ¬å–å·¥å…·")
    print("=" * 80)

    # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²è¿è¡Œ
    server_process = None
    if check_server_running():
        print(f"âœ… æœåŠ¡å™¨å·²åœ¨è¿è¡Œ: {BASE_URL}")
    else:
        try:
            server_process = start_server()
        except Exception as e:
            print(f"âŒ æ— æ³•å¯åŠ¨æœåŠ¡å™¨: {e}")
            sys.exit(1)

    try:
        # åˆ›å»ºçˆ¬è™«
        crawler = CompleteSiteCrawler()

        # å¼€å§‹çˆ¬å–
        print("\n" + "=" * 80)
        print("å¼€å§‹æ·±åº¦çˆ¬å–é¡µé¢...")
        print("=" * 80)

        crawler.crawl_bfs(BASE_URL)

        # ç”ŸæˆæŠ¥å‘Š
        crawler.generate_report()

        # æ€»ç»“
        elapsed_time = time.time() - start_time
        print("\n" + "=" * 80)
        print("çˆ¬å–å®Œæˆï¼")
        print("=" * 80)
        print(f"âœ… æˆåŠŸ: {len(crawler.crawled_urls)} ä¸ªé¡µé¢")
        print(f"âŒ å¤±è´¥: {len(crawler.failed_urls)} ä¸ªé¡µé¢")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    finally:
        # åœæ­¢æœåŠ¡å™¨
        if server_process:
            print("\nğŸ›‘ åœæ­¢æœåŠ¡å™¨...")
            server_process.terminate()
            try:
                server_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                server_process.kill()
            print("âœ… æœåŠ¡å™¨å·²åœæ­¢")


if __name__ == "__main__":
    main()
