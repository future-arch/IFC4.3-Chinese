#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ curl æ‰¹é‡ä¸‹è½½é™æ€é¡µé¢

ä» urls.txt è¯»å–æ‰€æœ‰ URLï¼Œä½¿ç”¨ curl ä¸‹è½½å¹¶ä¿å­˜ä¸ºé™æ€ HTML æ–‡ä»¶
æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è¿›åº¦æŠ¥å‘Šã€é”™è¯¯é‡è¯•
"""

import os
import sys
import time
import subprocess
from pathlib import Path
from urllib.parse import urlparse, unquote
from datetime import datetime

# é…ç½®
BASE_URL = "http://127.0.0.1:5050"
OUTPUT_DIR = Path("/Users/weilai/Documents/devs/IFC-4-3-Chinese")
URL_LIST = Path("/Users/weilai/Documents/devs/IFC-4-3-Chinese/deployment/urls.txt")
MAX_RETRIES = 3
REPORT_INTERVAL = 50  # æ¯ä¸‹è½½50ä¸ªé¡µé¢æŠ¥å‘Šä¸€æ¬¡


class CurlDownloader:
    """åŸºäº curl çš„é¡µé¢ä¸‹è½½å™¨"""

    def __init__(self):
        self.total_urls = 0
        self.downloaded = 0
        self.failed = []
        self.skipped = 0
        self.start_time = time.time()

    def get_file_path(self, url: str) -> Path:
        """
        æ ¹æ® URL ç¡®å®šæ–‡ä»¶ä¿å­˜è·¯å¾„

        Args:
            url: é¡µé¢ URL

        Returns:
            æ–‡ä»¶ä¿å­˜è·¯å¾„
        """
        parsed = urlparse(url)
        url_path = parsed.path

        # æ ¹ç›®å½•
        if url_path == "/" or url_path == "":
            return OUTPUT_DIR / "index.html"

        # ç§»é™¤å‰å¯¼æ–œæ 
        url_path = url_path.lstrip("/")
        # URL è§£ç 
        url_path = unquote(url_path)

        # å¦‚æœä»¥ .htm æˆ– .html ç»“å°¾ï¼Œç›´æ¥ä½¿ç”¨
        if url_path.endswith((".htm", ".html")):
            return OUTPUT_DIR / url_path
        else:
            # å¦åˆ™åˆ›å»ºç›®å½•å¹¶ä¿å­˜ä¸º index.html
            return OUTPUT_DIR / url_path / "index.html"

    def should_skip(self, file_path: Path) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦åº”è·³è¿‡ä¸‹è½½
        """
        if not file_path.exists():
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆå¤ªå°å¯èƒ½æ˜¯é”™è¯¯é¡µé¢ï¼‰
        if file_path.stat().st_size < 100:
            return False

        return True

    def download_url(self, url: str, retry: int = 0) -> bool:
        """
        ä½¿ç”¨ curl ä¸‹è½½å•ä¸ª URL

        Args:
            url: é¡µé¢ URL
            retry: é‡è¯•æ¬¡æ•°

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        file_path = self.get_file_path(url)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self.should_skip(file_path):
            self.skipped += 1
            return True

        # åˆ›å»ºç›®å½•
        file_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            # ä½¿ç”¨ curl ä¸‹è½½
            result = subprocess.run(
                [
                    "curl",
                    "-s",  # é™é»˜æ¨¡å¼
                    "-f",  # å¤±è´¥æ—¶è¿”å›é”™è¯¯
                    "-L",  # è·Ÿéšé‡å®šå‘
                    "--max-time", "30",  # è¶…æ—¶ 30 ç§’
                    "-o", str(file_path),  # è¾“å‡ºæ–‡ä»¶
                    url
                ],
                capture_output=True,
                text=True,
                timeout=35
            )

            if result.returncode == 0:
                # éªŒè¯æ–‡ä»¶å¤§å°
                if file_path.stat().st_size < 100:
                    raise ValueError("ä¸‹è½½çš„æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢")

                self.downloaded += 1
                return True
            else:
                raise subprocess.CalledProcessError(
                    result.returncode,
                    "curl",
                    stderr=result.stderr
                )

        except Exception as e:
            if retry < MAX_RETRIES:
                time.sleep(2 ** retry)  # æŒ‡æ•°é€€é¿
                return self.download_url(url, retry + 1)
            else:
                print(f"  âŒ å¤±è´¥: {url}")
                print(f"     é”™è¯¯: {e}")
                self.failed.append((url, str(e)))
                return False

    def print_progress(self):
        """æ‰“å°ä¸‹è½½è¿›åº¦"""
        elapsed = time.time() - self.start_time
        processed = self.downloaded + self.skipped + len(self.failed)

        if processed > 0:
            speed = processed / elapsed if elapsed > 0 else 0
            eta = (self.total_urls - processed) / speed if speed > 0 else 0
        else:
            speed = 0
            eta = 0

        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¿›åº¦æŠ¥å‘Š [{datetime.now().strftime('%H:%M:%S')}]")
        print(f"{'='*80}")
        print(f"  æ€» URL æ•°: {self.total_urls}")
        print(f"  âœ… å·²ä¸‹è½½: {self.downloaded}")
        print(f"  â­ï¸  å·²è·³è¿‡: {self.skipped} (æ–‡ä»¶å·²å­˜åœ¨)")
        print(f"  âŒ å¤±è´¥: {len(self.failed)}")
        print(f"  ğŸ“ˆ å·²å¤„ç†: {processed}/{self.total_urls} ({processed/self.total_urls*100:.1f}%)")
        print(f"  â±ï¸  å·²ç”¨æ—¶: {elapsed:.1f} ç§’")
        print(f"  ğŸš€ é€Ÿåº¦: {speed:.1f} é¡µ/ç§’")
        print(f"  â° é¢„è®¡å‰©ä½™: {eta:.0f} ç§’")
        print(f"{'='*80}\n")

    def download_all(self, url_list_file: Path):
        """
        ä¸‹è½½æ‰€æœ‰ URL

        Args:
            url_list_file: URL åˆ—è¡¨æ–‡ä»¶è·¯å¾„
        """
        # è¯»å– URL åˆ—è¡¨
        with open(url_list_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]

        self.total_urls = len(urls)

        print(f"{'='*80}")
        print(f"å¼€å§‹æ‰¹é‡ä¸‹è½½é¡µé¢")
        print(f"{'='*80}")
        print(f"  æ€» URL æ•°: {self.total_urls}")
        print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        print(f"  æ–­ç‚¹ç»­ä¼ : å·²å¯ç”¨ï¼ˆè·³è¿‡å·²å­˜åœ¨æ–‡ä»¶ï¼‰")
        print(f"{'='*80}\n")

        # é€ä¸ªä¸‹è½½
        for i, url in enumerate(urls, 1):
            self.download_url(url)

            # æ¯éš”ä¸€å®šæ•°é‡æŠ¥å‘Šè¿›åº¦
            if i % REPORT_INTERVAL == 0:
                self.print_progress()

        # æœ€ç»ˆæŠ¥å‘Š
        self.print_progress()

        # ç”Ÿæˆå¤±è´¥æŠ¥å‘Š
        if self.failed:
            report_path = OUTPUT_DIR / "deployment" / "download_failed.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"ä¸‹è½½å¤±è´¥çš„ URL ({len(self.failed)} ä¸ª)\n")
                f.write("="*80 + "\n\n")
                for url, error in self.failed:
                    f.write(f"URL: {url}\n")
                    f.write(f"é”™è¯¯: {error}\n\n")
            print(f"âŒ å¤±è´¥æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def check_server_running() -> bool:
    """æ£€æŸ¥ Flask æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    try:
        result = subprocess.run(
            ["curl", "-s", "-f", BASE_URL],
            capture_output=True,
            timeout=5
        )
        return result.returncode == 0
    except:
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("IFC 4.3 ä¸­æ–‡ç‰ˆ - æ‰¹é‡é¡µé¢ä¸‹è½½å·¥å…·")
    print("="*80)

    # æ£€æŸ¥æœåŠ¡å™¨
    if not check_server_running():
        print(f"âŒ Flask æœåŠ¡å™¨æœªè¿è¡Œ: {BASE_URL}")
        print("   è¯·å…ˆå¯åŠ¨æœåŠ¡å™¨")
        sys.exit(1)

    print(f"âœ… æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ: {BASE_URL}\n")

    # æ£€æŸ¥ URL åˆ—è¡¨
    if not URL_LIST.exists():
        print(f"âŒ URL åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {URL_LIST}")
        sys.exit(1)

    # å¼€å§‹ä¸‹è½½
    downloader = CurlDownloader()
    downloader.download_all(URL_LIST)

    # æ€»ç»“
    print("\n" + "="*80)
    print("ä¸‹è½½å®Œæˆï¼")
    print("="*80)
    print(f"  âœ… æˆåŠŸä¸‹è½½: {downloader.downloaded} ä¸ªé¡µé¢")
    print(f"  â­ï¸  è·³è¿‡: {downloader.skipped} ä¸ªé¡µé¢ (å·²å­˜åœ¨)")
    print(f"  âŒ å¤±è´¥: {len(downloader.failed)} ä¸ªé¡µé¢")
    print(f"  â±ï¸  æ€»è€—æ—¶: {time.time() - downloader.start_time:.1f} ç§’")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("="*80)


if __name__ == "__main__":
    main()
